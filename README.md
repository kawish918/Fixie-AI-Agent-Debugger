# üîß Fixie: AI Agent Debugger

An intelligent Python debugging assistant powered by AI agents using LangChain and Ollama Llama 3.2. Fixie automatically detects bugs, analyzes code logic, and suggests fixes through a coordinated multi-agent workflow.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-ü¶úüîó-green.svg)
![Ollama](https://img.shields.io/badge/Ollama-Llama3.2-orange.svg)

## ‚ú® Features

- ü§ñ **Multi-Agent Architecture**: Specialized agents for syntax checking, logic reasoning, and fix suggestions
- üîç **Intelligent Bug Detection**: Identifies syntax errors, runtime issues, and logic problems
- üìç **Line-by-Line Analysis**: Pinpoints exact locations of bugs with line numbers
- üéØ **Confidence Scoring**: Provides confidence levels for suggested fixes
- üîÑ **LangGraph Workflow**: Orchestrated agent coordination using LangGraph
- ü¶ô **Local AI**: Powered by Ollama Llama 3.2 for privacy and offline usage

## üöÄ Quick Start

### Prerequisites

- Python 3.8+
- Ollama installed and running
- Git (for cloning)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/kawish918/Fixie-AI-Agent-Debugger.git
   cd fixie-ai-debugger
   ```

2. **Create virtual environment**
   ```bash
   python -m venv fixie
   fixie\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install langchain langgraph
   ```

4. **Install and setup Ollama**
   
   Visit [Ollama's official website](https://ollama.ai) for installation instructions, or:
   
   **Windows/Mac/Linux:**
   ```bash
   # Download from: https://ollama.ai/download
   # Then pull the Llama model:
   ollama pull llama3.2
   ```

5. **Verify Ollama is running**
   ```bash
   ollama list
   # Should show llama3.2 in the list
   ```

### Usage

1. **Run on the example buggy code**
   ```bash
   python main.py
   ```

2. **Add your own buggy Python files**
   ```bash
   # Place your .py file in the examples/ directory
   # Update main.py to point to your file:
   code = read_python_file('examples/your_buggy_file.py')
   ```

## üìÅ Project Structure

```
fixie-ai-debugger/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ fix_suggester.py      # AI agent for generating fixes
‚îÇ   ‚îú‚îÄ‚îÄ logic_reasoner.py     # AI agent for understanding code logic
‚îÇ   ‚îî‚îÄ‚îÄ syntax_checker.py     # AI agent for detecting syntax errors
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ agent_runner.py       # Simple agent orchestration
‚îÇ   ‚îú‚îÄ‚îÄ input_handler.py      # File reading utilities
‚îÇ   ‚îú‚îÄ‚îÄ langgraph_runner.py   # LangGraph workflow management
‚îÇ   ‚îî‚îÄ‚îÄ llama_interface.py    # Ollama API interface
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ buggy_code.py         # Sample buggy code for testing
‚îú‚îÄ‚îÄ main.py                   # Main application entry point
‚îî‚îÄ‚îÄ README.md
```

## üîÑ How It Works

Fixie uses a coordinated multi-agent workflow:

1. **Syntax Checker Agent** üîç
   - Analyzes code for syntax and runtime errors
   - Identifies problematic lines and severity levels

2. **Logic Reasoner Agent** üß†
   - Understands the intended purpose of the code
   - Provides context for fix generation

3. **Fix Suggester Agent** üõ†Ô∏è
   - Combines bug report and logic analysis
   - Generates complete, executable fix suggestions
   - Provides confidence scores

4. **LangGraph Orchestration** üîÑ
   - Manages agent workflow and data flow
   - Ensures proper sequencing and state management

## üìä Example Output

```
--- Fixie AI Debugger ---

Original Code:
def add_nums(a, b):
    return a + b + c

üîç Debug Results:
==================================================
üêõ Bug Found: NameError - variable 'c' is not defined
üìç Line Number: 2
‚ö†Ô∏è  Severity: HIGH
--------------------------------------------------
üìä Fix Confidence: 0.95
üí° Explanation: Variable 'c' is undefined in the function
üîß Suggested Fix:
def add_nums(a, b):
    return a + b
```

## ‚öôÔ∏è Configuration

### Using Different Models

Update the model in any agent:

```python
# In agents/fix_suggester.py
class FixSuggester:
    def __init__(self, model="llama3.2"):  # Change model here
        self.model = model
```

### Customizing Prompts

Modify prompts in each agent class to suit your debugging needs:

```python
# In agents/syntax_checker.py
def check(self, code: str) -> dict:
    prompt = f"""Your custom prompt here..."""
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìã Future Enhancements

- [ ] Support for multiple programming languages
- [ ] Integration with popular IDEs
- [ ] Web interface for easier usage
- [ ] Code execution validation
- [ ] Integration with static analysis tools
- [ ] Custom rule definitions
- [ ] Batch processing of multiple files

## üîß Troubleshooting

### Common Issues

**Ollama not found:**
```bash
# Make sure Ollama is installed and in PATH
ollama --version
```

**Model not available:**
```bash
# Pull the required model
ollama pull llama3.2
```

**LangChain import errors:**
```bash
# Install missing dependencies
pip install langchain langgraph
```

**Permission errors on Windows:**
```bash
# Run as administrator or check antivirus settings
```

## üìö Resources

- [LangChain Documentation](https://docs.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Llama 3.2 Model Card](https://ollama.ai/library/llama3.2)
- [Get LangSmith API Key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
- [LangGraph Local Server](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

##  Acknowledgments

- [LangChain](https://langchain.com/) for the agent framework
- [Ollama](https://ollama.ai/) for local AI model hosting
- [Meta](https://ai.meta.com/) for the Llama 3.2 model

---

*Star ‚≠ê this repo if you find it helpful!*
